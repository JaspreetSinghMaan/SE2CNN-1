{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Erik J. Bekkers and Maxime W. Lafarge, Eindhoven University of Technology, the Netherlands*\n",
    "\n",
    "*8 June 2018*\n",
    "\n",
    "***\n",
    "\n",
    "*This DEMO was tested on a laptop with*:\n",
    "- *Windows as OS*\n",
    "- *Jupyter Notebook (version 5.5.0)*\n",
    "- *Python (version 3.5.5)*\n",
    "- *TensorFlow-GPU (versions 1.1 and higher)*\n",
    "- *An NVIDIA Quadro M1000M GPU*\n",
    "- *The following additional libraries installed for this demo to run: sklearn, scipy, and matplotlib*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage of the se2cnn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter demo will contain the basic usage examples of the se2cnn library with applications to digit recognition in the MNIST dataset. The se2cnn library contains 3 main layers (check the useage via *help(se2cnn.layers.z2_se2n)*):\n",
    "- **z2_se2n**: a lifting layer from 2D tensor to SE(2) tensor\n",
    "- **se2n_se2n**: a group convolution layer from SE(2) tensor to SE(2) tensor\n",
    "- **spatial_max_pool**: performs 2x2 spatial max-pooling of the spatial axes\n",
    "\n",
    "The following functions are used internally, but may be of interest as well:\n",
    "- **rotate_lifting_kernels**: rotates the raw 2D lifting kernels (output is a set of rotated kernels)\n",
    "- **rotate_gconv_kernels**: rotates (shift-twists) the se2 kernels (planar rotation + orientation shift)\n",
    "\n",
    "\n",
    "\n",
    "In this demo we will construct the following network:\n",
    "\n",
    "| layer nr. | Layer                          | Tensor shape  |\n",
    "| --------- | ------------------------------ | ------------- |\n",
    "| 0         | input                          | (32 x 32 x 1) |\n",
    "| --------- | --------------------------------------------------- | -------------------------- |\n",
    "| 1         | 5x5 lifting convultion (+ReLU) | (28 x 28 x Ntheta x Nc)  |\n",
    "| 1         | 2x2 max pooling                | (14 x 14 x Ntheta x Nc)  |\n",
    "| --------- | --------------------------------------------------- | -------------------------- |\n",
    "| 2         | 5x5 group convultion (+ReLU)   | (10 x 10 x Ntheta x 2*Nc)  |\n",
    "| 2         | 2x2 max pooling                | (5 x 5 x Ntheta x 2*Nc)  |\n",
    "| --------- | --------------------------------------------------- | -------------------------- |\n",
    "| 3         | merge orientation+channel dim  | (5 x 5 x Ntheta 2*Nc) |\n",
    "| 3         | 5x5 2D convultion (+ReLU)      | (1 x 1 x 4*Nc)  |\n",
    "| --------- | --------------------------------------------------- | -------------------------- |\n",
    "| 4         | 1x1 2D convultion (+ReLU)      | (1 x 1 x 128)  |\n",
    "| --------- | --------------------------------------------------- | -------------------------- |\n",
    "| 5         | 1x1 2D convolution: the output layer | (10) |\n",
    "\n",
    "Here Ntheta is the number of orientation samples to discretize the SE(2) group, Nc is the number of channels in the lifting layer. The first two layers are **roto-translation covariant**, meaning that the feature vectors rotate according to rotations of the input patterns (e.g. basic features do not need to be learned for each orientation). In layer 3 the orientation axis is merged with the channel axis, followed by 2D convolutions, making the subsequent layers **only translation covariant**. Here we made this choice due to the fact that in the MNIST dataset the characters always appear under the same orientation, and the task does not need to be rotation invariant.\n",
    "\n",
    "There are several options to make a network completely roto-translation invariant:\n",
    "1. For example one could stick with (5x5 and 1x1) SE(2) group convolutions all the way to the output layer, which would then provide a length 10 feature vector for each orientation. On can then simply do a maximum projection over the orientations (tf.reduce_max) to get the maximal response for each bin, followed by a softmax.\n",
    "2. One coulde reduce the patch size to [1 x 1 x Ntheta x Nc] via group convolutions, apply a maximum projection over theta, and perform 1x1 (fully connected) 2D convolutions all the way to the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 1: Load the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impor tensorflow and numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math as m\n",
    "import time\n",
    "# For validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# For plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Add the library to the system path\n",
    "import os,sys\n",
    "se2cnn_source =  os.path.join(os.getcwd(),'..')\n",
    "if se2cnn_source not in sys.path:\n",
    "    sys.path.append(se2cnn_source)\n",
    "\n",
    "# Import the library\n",
    "import se2cnn.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The se2cnn layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For useage of the relevant layers defined in se2cnn.layers uncomment and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(se2cnn.layers.z2_se2n)\n",
    "# help(se2cnn.layers.se2n_se2n)\n",
    "# help(se2cnn.layers.spatial_max_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initialization we use the initialization method for ReLU activation functions as proposed in:\n",
    "\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier's/He-Rang-Zhen-Sun initialization for layers that are followed ReLU\n",
    "def weight_initializer(n_in, n_out):\n",
    "    return tf.random_normal_initializer(mean=0.0, stddev=m.sqrt(2.0 / (n_in))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of a tf tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of(tensor) :\n",
    "    # Multiply elements one by one\n",
    "    result = 1\n",
    "    for x in tensor.get_shape().as_list():\n",
    "         result = result * x \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 2: Load and format the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIS dataset consists of 28x28 images of handwritten characters. We are going to classify each input image into 1 of the 10 classes (the digits 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the data is formatted as flattened arrays. Here were format them as 2D feature maps (with only 1 channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x75020369b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape to 2D multi-channel images\n",
    "train_data_2D = train_data.reshape([len(train_data),28,28,1]) # [batch_size, Nx, Ny, Nc]\n",
    "eval_data_2D = eval_data.reshape([len(eval_data),28,28,1])\n",
    "\n",
    "# Plot the first sample\n",
    "plt.plot()\n",
    "plt.title('Digit = %d' % train_labels[0])\n",
    "plt.imshow(train_data_2D[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have the patches to be of size 32x32 such that we can reduce it to 1x1 via 5x5 convolutions and max pooling layers. So, here we pad the images on the left and right with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2D = np.pad(train_data_2D,((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=((0,0),(0,0),(0,0),(0,0)))\n",
    "eval_data_2D = np.pad(eval_data_2D,((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=((0,0),(0,0),(0,0),(0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 3: Build a graph (design the G-CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel size and number of orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntheta = 12 # Kernel size in angular direction\n",
    "Nxy=5       # Kernel size in spatial direction\n",
    "Nc = 4      # Number of channels in the initial layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ph = tf.placeholder( dtype = tf.float32, shape = [None,32,32,1] )\n",
    "labels_ph = tf.placeholder( dtype = tf.int32, shape = [None,] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_in = inputs_ph\n",
    "Nc_in = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the kernels to a library for later inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: The lifting layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2-SE2N BASE KERNEL SHAPE: (5, 5, 1, 4)\n",
      "Z2-SE2N ROTATED KERNEL SET SHAPE: (12, 5, 5, 1, 4)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, 28, 28, 12, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(1)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = Nc\n",
    "\n",
    "    ## Perform lifting convolution\n",
    "    # The kernels used in the lifting layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # Lifting layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.z2_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw,\n",
    "                            orientations_nb = Ntheta)\n",
    "    # Add bias\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Perform (spatial) max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(12), Dimension(4)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: SE2-conv, max-pool, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE2N-SE2N BASE KERNEL SHAPE: (5, 5, 12, 4, 8)\n",
      "SE2N-SE2N ROTATED KERNEL SET SHAPE: (12, 5, 5, 12, 4, 8)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, 10, 10, 12, 8)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(2)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 2*Nc\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Ntheta,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Ntheta*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # The group convolution layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.se2n_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw)\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Perform max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(5), Dimension(5), Dimension(12), Dimension(8)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: 2D fully connected layer (5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the orientation and channel dimension\n",
    "tensor_in = tf.concat([tensor_in[:,:,:,i,:] for i in range(Ntheta)],3)\n",
    "Nc_in = tensor_in.get_shape().as_list()[-1]\n",
    "\n",
    "# 2D convolution layer\n",
    "with tf.variable_scope(\"Layer_{}\".format(3)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 4*Nc\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # Convolution layer\n",
    "    tensor_out = tf.nn.conv2d(\n",
    "                        input = tensor_in,\n",
    "                        filter=kernels_raw,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\")\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(16)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Fully connected layer (1x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D convolution layer\n",
    "with tf.variable_scope(\"Layer_{}\".format(4)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 128\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [1,1,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(1*1*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # Convolution layer\n",
    "    tensor_out = tf.nn.conv2d(\n",
    "                        input = tensor_in,\n",
    "                        filter=kernels_raw,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\")\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(128)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: Fully connected (1x1) to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(5)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 10\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [1,1,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(1*1*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "\n",
    "    \n",
    "    ## Convolution layer\n",
    "    tensor_out = tf.nn.conv2d(\n",
    "                        input = tensor_in,\n",
    "                        filter=kernels_raw,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\")\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## The output logits\n",
    "    logits = tensor_out[:,0,0,:]\n",
    "    predictions = tf.argmax(input=logits, axis=1)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    ## Save the kernels for later inspection\n",
    "    kernels[_scope.name] = kernels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels_ph, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "RAW kernel shapes:\n",
      "[Layer_1/kernel:0]: (5, 5, 1, 4), total nr of weights = 100\n",
      "[Layer_2/kernel:0]: (5, 5, 12, 4, 8), total nr of weights = 9600\n",
      "[Layer_3/kernel:0]: (5, 5, 96, 16), total nr of weights = 38400\n",
      "[Layer_4/kernel:0]: (1, 1, 16, 128), total nr of weights = 2048\n",
      "[Layer_5/kernel:0]: (1, 1, 128, 10), total nr of weights = 1280\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#-- Define the l2 loss \n",
    "weightDecay=5e-4\n",
    "# Get the raw kernels\n",
    "variables_wd = tf.get_collection('raw_kernels')\n",
    "print('-----')\n",
    "print('RAW kernel shapes:')\n",
    "for v in variables_wd: print( \"[{}]: {}, total nr of weights = {}\".format(v.name, v.get_shape(), size_of(v)))\n",
    "print('-----')\n",
    "loss_l2 = weightDecay*sum([tf.nn.l2_loss(ker) for ker in variables_wd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Training Op (for TRAIN mode)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss + loss_l2,\n",
    "    global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 4: Train and test the G-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Start the (GPU) session\n",
    "initializer = tf.global_variables_initializer()\n",
    "session = tf.Session(graph=tf.get_default_graph()) #-- Session created\n",
    "session.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each epoch we pass over all input samples in batch sizes of batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "n_epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the input stack in batch of size \"batch_size\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  finished... Average loss =  0.1842 , time =  23.8895\n",
      "Epoch  1  finished... Average loss =  0.0518 , time =  22.2277\n",
      "Epoch  2  finished... Average loss =  0.0386 , time =  22.2963\n",
      "Epoch  3  finished... Average loss =  0.0289 , time =  22.2516\n",
      "Epoch  4  finished... Average loss =  0.0239 , time =  22.2692\n",
      "Epoch  5  finished... Average loss =  0.0206 , time =  22.0808\n",
      "Epoch  6  finished... Average loss =  0.018 , time =  22.0596\n",
      "Epoch  7  finished... Average loss =  0.0166 , time =  23.0784\n",
      "Epoch  8  finished... Average loss =  0.0156 , time =  22.6192\n",
      "Epoch  9  finished... Average loss =  0.013 , time =  22.7916\n"
     ]
    }
   ],
   "source": [
    "for epoch_nr in range(n_epochs):\n",
    "    loss_average = 0\n",
    "    data = train_data_2D\n",
    "    labels = train_labels\n",
    "    # KBatch settings\n",
    "    NItPerEpoch = m.floor(len(data)/batch_size) #number of iterations per epoch\n",
    "    samples=np.random.permutation(len(data))\n",
    "    # Loop over dataset\n",
    "    tStart = time.time()\n",
    "    for iteration in range(NItPerEpoch):\n",
    "        feed_dict = {\n",
    "                inputs_ph: np.array(data[samples[iteration*batch_size:(iteration+1)*batch_size]]),\n",
    "                labels_ph: np.array(labels[samples[iteration*batch_size:(iteration+1)*batch_size]])\n",
    "                }\n",
    "        operators_output = session.run([ loss , train_op ], feed_dict)\n",
    "        loss_average += operators_output[0]/NItPerEpoch\n",
    "    tElapsed = time.time() - tStart\n",
    "    print('Epoch ' , epoch_nr , ' finished... Average loss = ' , round(loss_average,4) , ', time = ',round(tElapsed,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "labels_pred = []\n",
    "for i in range(round(len(eval_data_2D)/batch_size)):\n",
    "    [ labels_pred_batch ] = session.run([ predictions ], { inputs_ph: eval_data_2D[i*batch_size:(i+1)*batch_size] })\n",
    "    labels_pred = labels_pred + list(labels_pred_batch)\n",
    "labels_pred = np.array(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the first 10 results with the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9]\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "print(labels_pred[0:10])\n",
    "print(eval_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy (average nr of successes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((labels_pred - eval_labels)**2==0).astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total nr of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((labels_pred - eval_labels)**2>0).astype(float).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*((labels_pred - eval_labels)**2>0).astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix to see what kind of errors are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(eval_labels, labels_pred)\n",
    "plot_confusion_matrix(cm, range(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
